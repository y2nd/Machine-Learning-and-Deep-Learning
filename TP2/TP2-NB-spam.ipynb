{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IF240 - Machine Learning and Deep Learning\n",
    "\n",
    "## TP 2 - Naive Bayes and Evaluation Metrics\n",
    "\n",
    "By Micha√´l Cl√©ment and Aur√©lie Bugeau\n",
    "\n",
    "Credits:  Vincent Lepetit, Varun Kumar, Mohit Deshpande"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objectives \n",
    "The objective of the practice is to clssifiy emails from a dataset as spam or non-spam.\n",
    "\n",
    "You will implement the Naive Bayes classifier, and test the model with several validation metrics.\n",
    "\n",
    "### Libraries\n",
    "\n",
    "The code needs to import the following libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import re\n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Dataset\n",
    "\n",
    "#### Presentation and Loading\n",
    "The dataset used here contained 747 spam and 4825 non-spam (i.e. ham) mails. \n",
    "Emails in the corpus have been already pre-processed in the following ways:\n",
    "\n",
    "- Removal of stop words (and, the, of, etc)\n",
    "- Lemmatization (inludes, included, include are now all considered as include)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ham     4825\n",
      "spam     747\n",
      "Name: Category, dtype: int64\n",
      "  Category                                            Message\n",
      "0      ham  Go until jurong point, crazy.. Available only ...\n",
      "1      ham                      Ok lar... Joking wif u oni...\n",
      "2     spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
      "3      ham  U dun say so early hor... U c already then say...\n",
      "4      ham  Nah I don't think he goes to usf, he lives aro... \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Open the dataset and count the number of spam/ham mails\n",
    "mails = pd.read_csv(\"spamham.csv\")\n",
    "count = mails['Category'].value_counts()\n",
    "print(count)\n",
    "print(mails.head(), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training and evaluation sets\n",
    "Split the dataset into training and evaluation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data as train and evaluation sets\n",
    "msk = np.random.rand(len(mails)) < 0.8\n",
    "training_set = mails[msk]\n",
    "testing_set = mails[~msk]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Naive Bayes Classification\n",
    "The classifier must be able to predict the label based on the text by implementing the following pseudo code:\n",
    "\n",
    "`if (P('ham' | message ) > P( 'spam' | message )) return ‚Äòham‚Äô\n",
    "else return ‚Äòspam‚Äô`\n",
    "\n",
    "where\n",
    "$$ P(ham | message)~=~ {\\rm Probability ~that~ email~ is ~ham~ given~ that~ it~ has~ certain~ features~} $$\n",
    "$$ P(spam | message)~=~ {\\rm Probability ~that~ email~ is ~spam~ given~ that~ it~ has~ certain~ features~} $$\n",
    "\n",
    "The features will be based on the number of occurence of each word in the message.\n",
    "\n",
    "(See the bag-of-words model: https://en.wikipedia.org/wiki/Bag-of-words_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1 \n",
    "\n",
    "Apply the Naive Bayes formula in the following code to implement a classifier. You will consider that:\n",
    "$$P(message | spam) = P(word1 | spam) * P(word2 | spam) *...$$\n",
    "\n",
    " \n",
    "_Note:_ if a word in the testing dataset is not present in the training dataset, you may encounter problems as $P(new | ham)$ or $P(new | spam)$ will be 0 making all product equal to 0.\n",
    "To solve this problem, we should take log on both sides. New pseudo code will be\n",
    "\n",
    "`if (log(P('ham' | message )) > log(P('spam' | message))) return ‚Äòham‚Äô\n",
    "else return ‚Äòspam‚Äô`\n",
    "\n",
    "Then \n",
    "$$ log(P(message| spam)) =  log(P(word1 | spam)) + log(P(word2 | spam)) ‚Ä¶$$\n",
    "\n",
    "But the problem is still not solved. If the classifier encounters a new word that is not present in our training data sets then P(new-word | category) will be 0 and log(0) is undefined. To solve this problem, you must use Laplace smoothing:\n",
    "\n",
    "$$P(word1 | spam) = \\frac{{\\rm number~ of ~}word1 {\\rm~belonging~ to ~category~ spam + 1}}{{\\rm  number ~ of ~words~ belonging~ to ~spam ~}+{ \\rm ~number ~of~ distinct ~words~ in ~training ~datasets~}}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenize a string into words    \n",
    "def tokenize(text):\n",
    "    return re.split(\"\\W+\", str(text))\n",
    "    \n",
    "class SpamDetectorNB(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.numberOfmessages = {} \n",
    "        self.log_class_priors = {}\n",
    "        self.word_counts = {}\n",
    "        self.word_index = {}\n",
    "        self.vocab = set()\n",
    "    \n",
    "\n",
    "    # compute log class priors log(ùëÉ(‚Ñéùëéùëö)) and log(ùëÉ(spùëéùëö))  \n",
    "    #by counting up how many spam/ham messages are in our dataset and dividing by the total number\n",
    "    def log_priors(self, training_set):\n",
    "        nb_ham = training_set['Category'].value_counts()['ham']\n",
    "        nb_spam = training_set['Category'].value_counts()['spam']\n",
    "        self.log_class_priors['spam'] = np.log(nb_spam / nb_ham + nb_spam)\n",
    "        self.log_class_priors['ham'] = np.log(nb_ham / nb_ham + nb_spam)\n",
    "             \n",
    "    #Count how many times each word appears in a text. \n",
    "    #Returns a dictionary that contain for each word indicates the number of times it appears in text. \n",
    "    def get_word_counts(self, text):\n",
    "        word_counts = {}\n",
    "        for w in tokenize(text):\n",
    "            word_counts[w] = word_counts.get(w, 0) + 1\n",
    "        #You can use the Python dictionary method get()\n",
    "        return word_counts\n",
    "    \n",
    "    #Create a dictionary (a vocabulary of words)\n",
    "    #and count words frequency for spam and ham separately\n",
    "    def get_word_frequency(self, training_set):\n",
    "        self.word_counts['spam'] = {}\n",
    "        self.word_counts['ham'] = {}\n",
    "        for _, mail in training_set.iterrows():\n",
    "            label = mail['Category']\n",
    "            text = mail['Message']\n",
    "            #Tokenize each message into words.\n",
    "            counts = self.get_word_counts(tokenize(text))\n",
    "            for word, count in counts.items():\n",
    "                if word not in self.vocab:\n",
    "                    self.word_index[word] = len(self.vocab)\n",
    "                    self.vocab.add(word)\n",
    "                if word not in self.word_counts[label]:\n",
    "                    self.word_counts[label][word] = 0.0\n",
    "                self.word_counts[label][word] += count\n",
    "                \n",
    "                \n",
    "    #compute all necessary features\n",
    "    def train(self, training_set):\n",
    "        self.log_priors(training_set)\n",
    "        self.get_word_frequency(training_set)\n",
    "        \n",
    "        \n",
    "    def predict(self, testing_set):\n",
    "        result = []\n",
    "        for _, mail in testing_set.iterrows():\n",
    "            label = mail['Category']\n",
    "            text = mail['Message']\n",
    "            \n",
    "            #Tokenize each message into words.\n",
    "            counts = self.get_word_counts(tokenize(text))\n",
    "            \n",
    "            #Initialize ùëôùëúùëî(ùëÉ(spam|message)) and ùëôùëúùëî(ùëÉ(ham|message))  according to log priors\n",
    "            log_spam = self.log_class_priors['spam']\n",
    "            log_ham = self.log_class_priors['ham']\n",
    "            \n",
    "            #For each message, compute ùëôùëúùëî(ùëÉ(ùëöùëíùë†ùë†ùëéùëîùëí|ùë†ùëùùëéùëö)) and ùëôùëúùëî(ùëÉ(ùëöùëíùë†ùë†ùëéùëîùëí|ùë†ùëùùëéùëö)) \n",
    "            for word, _ in counts.items():\n",
    "                if word not in self.vocab: continue         \n",
    "                #For each word compute log(P(w/spam)) and log(P(w/ham))\n",
    "                occurrences_in_spam = self.word_counts['spam'].get(word, 0)\n",
    "                log_w_spam = np.log(occurrences_in_spam + 1 / (len(self.word_counts['spam']) + len(self.vocab)))\n",
    "                \n",
    "                occurrences_in_ham = self.word_counts['ham'].get(word, 0)\n",
    "                log_w_ham = np.log(occurrences_in_ham + 1 / (len(self.word_counts['ham']) + len(self.vocab)))\n",
    "                \n",
    "                #Update ùëôùëúùëî(ùëÉ(ùëöùëíùë†ùë†ùëéùëîùëí|ùë†ùëùùëéùëö)) and ùëôùëúùëî(ùëÉ(ùëöùëíùë†ùë†ùëéùëîùëí|ùë†ùëùùëéùëö)) \n",
    "                log_spam += log_w_spam\n",
    "                log_ham += log_w_ham\n",
    "                \n",
    "                \n",
    "            #decide spam or ham\n",
    "            if(log_ham > log_spam):\n",
    "                result.append([label, 'ham'])\n",
    "            else:\n",
    "                result.append([label, 'spam'])            \n",
    "\n",
    "        return result             \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the classifier to the spam dataset\n",
    "sd = SpamDetectorNB()\n",
    "sd.train(training_set)\n",
    "result = sd.predict(testing_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2 - Accuracy and confusion matrix\n",
    "Compute the precision, recall, accuracy and confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision for spam: 0.9835\n",
      "Precision for ham: 0.9782\n",
      "Recall for spam: 0.8440\n",
      "Recall for ham: 0.9980\n",
      "Accuracy : 0.9787\n",
      "Confusion matrix using SpamDetectorNB:\n",
      "       spam    ham\n",
      "spam  119.0   22.0\n",
      "ham     2.0  986.0\n"
     ]
    }
   ],
   "source": [
    "#value at index = 0 is actual label and index = 1 is predicted label\n",
    "tp_spam = 0\n",
    "fp_spam = 0\n",
    "\n",
    "tp_ham = 0\n",
    "fp_ham = 0\n",
    "for i in range(len(result)):\n",
    "    if result[i][0] == result[i][1]:\n",
    "        if result[i][0] == 'spam':\n",
    "            tp_spam += 1\n",
    "        else:\n",
    "            tp_ham += 1\n",
    "    else:\n",
    "        if result[i][1] == 'spam':\n",
    "            fp_spam += 1\n",
    "        else:\n",
    "            fp_ham += 1\n",
    "\n",
    "precision_spam = tp_spam / (tp_spam + fp_spam)\n",
    "print(\"Precision for spam: {0:.4f}\".format(precision_spam))\n",
    "\n",
    "precision_ham = tp_ham / (tp_ham + fp_ham)\n",
    "print(\"Precision for ham: {0:.4f}\".format(precision_ham))\n",
    "\n",
    "# false negatives of spam == false positives of ham and vice versa\n",
    "recall_spam = tp_spam / (tp_spam + fp_ham)\n",
    "print(\"Recall for spam: {0:.4f}\".format(recall_spam))\n",
    "recall_ham = tp_ham / (tp_ham + fp_spam)\n",
    "print(\"Recall for ham: {0:.4f}\".format(recall_ham))\n",
    "\n",
    "\n",
    "accuracy = (tp_spam + tp_ham) / (tp_spam + tp_ham + fp_spam + fp_ham) \n",
    "print(\"Accuracy : {0:.4f}\".format(accuracy))\n",
    "\n",
    "print(\"Confusion matrix using SpamDetectorNB:\")\n",
    "confusion_mat = np.zeros((2, 2))\n",
    "confusion_mat[0, 0] = tp_spam\n",
    "confusion_mat[0, 1] = fp_ham # equal to false negatives of spam\n",
    "confusion_mat[1, 0] = fp_spam\n",
    "confusion_mat[1, 1] = tp_ham # equal to true negatives of spam\n",
    "\n",
    "classes = ['spam', 'ham']\n",
    "confusion_df = pd.DataFrame(confusion_mat, index=classes, columns=classes)\n",
    "print(confusion_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3 - Naive Bayes with Scikit-learn library\n",
    "The `scikit-learn` library proposes many functions for machine learning.  Study the documentation of the  `MultinomialNB` class and apply it for spam detection.\n",
    "\n",
    "You will need to convert the dataset into arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 1 1 1 1 0 1 1 1 1 1 1 1 1 1 0 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# preprocessing\n",
    "vectorizer = CountVectorizer()\n",
    "x = vectorizer.fit_transform(mails['Message']).toarray()\n",
    "y = 1 - LabelEncoder().fit_transform(mails['Category']) # spam = 0 and ham = 1\n",
    "\n",
    "# split train and test sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, random_state=0)\n",
    "\n",
    "# set up the MultinomialNB classifier and train it\n",
    "classifierMNB = MultinomialNB()\n",
    "classifierMNB.fit(x_train, y_train)\n",
    "\n",
    "# prediction\n",
    "y_pred = classifierMNB.predict(x_test)\n",
    "y_pred[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4 - Evaluation with Scikit-learn library\n",
    "\n",
    "The `scikit-learn` library also proposes  functions to evaluate machine learning methods.\n",
    "\n",
    "Apply them to the spam detection problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.9806\n",
      "\n",
      "Ou encore avec plus de d√©tails:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.93      0.93       185\n",
      "           1       0.99      0.99      0.99      1208\n",
      "\n",
      "    accuracy                           0.98      1393\n",
      "   macro avg       0.96      0.96      0.96      1393\n",
      "weighted avg       0.98      0.98      0.98      1393\n",
      "\n",
      "Confusion matrix using MultinomialNB:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report \n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred) \n",
    "print(\"Accuracy : {0:.4f}\".format(accuracy))\n",
    "\n",
    "print(\"\\nOu encore avec plus de d√©tails:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "print(\"Confusion matrix using MultinomialNB:\")\n",
    "cmMNB = confusion_matrix(y_test, y_pred)\n",
    "#for visualisation of the confusion matrix\n",
    "import seaborn as sns; sns.set()  # for plot styling\n",
    "sns.heatmap(cmMNB, square=True, annot=True, fmt='d', cbar=False, xticklabels = ['spam', 'ham']\\\n",
    "           ,yticklabels = ['spam', 'ham'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
